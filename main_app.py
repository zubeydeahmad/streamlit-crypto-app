# main_app.py
# (Ana Streamlit Uygulamasƒ±)
# Bu dosya Streamlit aray√ºz√ºn√º olu≈üturur, veri √ßekme, model eƒüitimi ve tahmin s√ºre√ßlerini entegre eder.
import streamlit as st
import pandas as pd
from datetime import datetime, timedelta, time # 'time' objesini import etmeyi unutmayƒ±n
import joblib
from sklearn.preprocessing import MinMaxScaler
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error
import numpy as np
import plotly.graph_objects as go
import logging
import os
import sys

# data_fetcher.py'nin bulunduƒüu dizini sys.path'e ekleyin
# (Proje yapƒ±sƒ±na g√∂re bu yolu ayarlamanƒ±z gerekebilir)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import data_fetcher as df # data_fetcher.py dosyasƒ±nƒ± df olarak import ediyoruz

# Loglama yapƒ±landƒ±rmasƒ±
logging.basicConfig(filename='crypto_app.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', encoding='utf-8')
logger = logging.getLogger(__name__)

# Streamlit sayfa yapƒ±landƒ±rmasƒ±
st.set_page_config(layout="wide", page_title="Finansal Varlƒ±k Analiz ve Tahmin Uygulamasƒ±", page_icon="üìà")

# Ba≈ülƒ±k
st.title("üìà Finansal Varlƒ±k Analiz ve Tahmin Uygulamasƒ±")
st.markdown("Bu uygulama ile √ße≈üitli finansal varlƒ±klarƒ±n ge√ßmi≈ü verilerini analiz edebilir, modelleyebilir ve gelecek fiyatlarƒ±nƒ± tahmin edebilirsiniz.")

# --- Sabitler ve Ayarlar ---
FEATURE_LAG = 7 # Modelin kullanacaƒüƒ± ge√ßmi≈ü g√ºn sayƒ±sƒ± (√∂zellikler i√ßin)
TARGET_LAG = 1 # 1 g√ºn sonrasƒ± i√ßin tahmin

# --- Yardƒ±mcƒ± Fonksiyonlar ---
# Caching key'e asset_symbol ve tarih aralƒ±klarƒ± ekleyerek, her farklƒ± √ßaƒürƒ±da yeniden veri √ßekilmesini saƒüla
@st.cache_data(show_spinner="Ge√ßmi≈ü veriler √ßekiliyor...", ttl=timedelta(hours=1))
def get_historical_data(asset_symbol: str, asset_source: str, start_date: datetime, end_date: datetime) -> pd.DataFrame:
    """Belirtilen varlƒ±k i√ßin ge√ßmi≈ü verileri √ßeker."""
    logger.info(f"get_historical_data √ßaƒürƒ±ldƒ±: Sembol={asset_symbol}, Kaynak={asset_source}, Ba≈ülangƒ±√ß={start_date.strftime('%Y-%m-%d')}, Biti≈ü={end_date.strftime('%Y-%m-%d')}")
    try:
        if asset_source == "yfinance":
            data = df.get_yfinance_data(asset_symbol, start_date, end_date)
        elif asset_source == "coinapi":
            # CoinAPI sembolleri genellikle BTC, ETH gibi 'base' currency formatƒ±ndadƒ±r.
            # Yfinance'daki BTC-USD gibi sembolleri burada da uyumlu hale getirmemiz gerekebilir.
            # ≈ûimdilik, CoinAPI i√ßin doƒürudan asset_symbol kullanacaƒüƒ±z.
            base_currency = asset_symbol.split('-')[0] if '-' in asset_symbol else asset_symbol
            data = df.get_coinapi_data(base_currency, days_back=(end_date - start_date).days)
        else:
            st.error("Bilinmeyen veri kaynaƒüƒ±.")
            return pd.DataFrame()
        
        logger.debug(f"get_historical_data: '{asset_symbol}' i√ßin √ßekilen veri bo≈ü mu? {data.empty}")
        if not data.empty:
            logger.debug(f"get_historical_data: '{asset_symbol}' i√ßin √ßekilen veri boyutu: {data.shape}")
            logger.debug(f"get_historical_data: '{asset_symbol}' i√ßin ilk 5 satƒ±r:\n{data.head()}")
        
        return data
    except Exception as e:
        logger.error(f"Ge√ßmi≈ü veri √ßekilirken hata olu≈ütu ({asset_symbol}): {e}")
        st.error(f"Ge√ßmi≈ü veri √ßekilirken hata olu≈ütu: {e}")
        return pd.DataFrame()

# Preprocessing ve Feature Engineering
@st.cache_data(show_spinner="Veriler i≈üleniyor...", ttl=timedelta(hours=1))
def preprocess_data(data: pd.DataFrame, asset_symbol: str) -> pd.DataFrame:
    """Veriye √∂zellik m√ºhendisliƒüi uygular ve temizler."""
    logger.info(f"preprocess_data √ßaƒürƒ±ldƒ± for {asset_symbol}. Gelen veri boyutu: {data.shape}")
    if data.empty:
        logger.warning(f"Preprocess for {asset_symbol}: Bo≈ü DataFrame geldi.")
        return pd.DataFrame()

    df_processed = data.copy()
    
    # Tarih indeksini saƒülamla≈ütƒ±r
    df_processed.index = pd.to_datetime(df_processed.index)
    df_processed = df_processed.sort_index()

    # Eksik g√ºnleri tamamla ve NaN deƒüerleri √∂nceki deƒüerle doldur
    # Bu adƒ±m, √∂zellikle yfinance'dan gelen ve haftasonu/tatil eksikliƒüi olan veriler i√ßin √∂nemli
    # Reindex yapmadan √∂nce en k√º√ß√ºk ve en b√ºy√ºk tarihi belirle
    min_date = df_processed.index.min()
    max_date = df_processed.index.max()
    full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')
    
    df_processed = df_processed.reindex(full_date_range)
    
    # ƒ∞leriye doƒüru doldur (ffill) ve sonra geriye doƒüru doldur (bfill)
    # Bu, herhangi bir ba≈ülangƒ±√ßtaki NaN'larƒ± da doldurur.
    df_processed.fillna(method='ffill', inplace=True)
    df_processed.fillna(method='bfill', inplace=True) # Ba≈ülangƒ±√ßtaki NaN'larƒ± doldurmak i√ßin

    # Ortalama Fiyat
    df_processed['Avg_Price'] = (df_processed['Open'] + df_processed['Close']) / 2

    # Hareketli Ortalamalar
    df_processed['MA7'] = df_processed['Close'].rolling(window=7).mean()
    df_processed['MA21'] = df_processed['Close'].rolling(window=21).mean()

    # RSI (G√∂receli G√º√ß Endeksi)
    # RSI hesaplamasƒ± i√ßin gerekli minimum veri: 14 periyot
    if len(df_processed) >= 14:
        delta = df_processed['Close'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df_processed['RSI'] = 100 - (100 / (1 + rs))
    else:
        df_processed['RSI'] = np.nan
        logger.warning(f"Preprocess for {asset_symbol}: RSI hesaplamak i√ßin yeterli veri yok (min 14 g√ºn gerekli).")

    # MACD
    # MACD hesaplamasƒ± i√ßin gerekli minimum veri: 26 periyot
    if len(df_processed) >= 26:
        exp1 = df_processed['Close'].ewm(span=12, adjust=False).mean()
        exp2 = df_processed['Close'].ewm(span=26, adjust=False).mean()
        df_processed['MACD'] = exp1 - exp2
        df_processed['Signal_Line'] = df_processed['MACD'].ewm(span=9, adjust=False).mean()
    else:
        df_processed['MACD'] = np.nan
        df_processed['Signal_Line'] = np.nan
        logger.warning(f"Preprocess for {asset_symbol}: MACD hesaplamak i√ßin yeterli veri yok (min 26 g√ºn gerekli).")

    # G√ºnl√ºk Getiri
    df_processed['Daily_Return'] = df_processed['Close'].pct_change()

    # Belirli bir gecikme i√ßin kaydƒ±rma
    # Son Fiyat (t-1), (t-2), ... (t-FEATURE_LAG)
    for i in range(1, FEATURE_LAG + 1):
        df_processed[f'Close_Lag_{i}'] = df_processed['Close'].shift(i)
        df_processed[f'Open_Lag_{i}'] = df_processed['Open'].shift(i)
        df_processed[f'High_Lag_{i}'] = df_processed['High'].shift(i)
        df_processed[f'Low_Lag_{i}'] = df_processed['Low'].shift(i)
        df_processed[f'Volume_Lag_{i}'] = df_processed['Volume'].shift(i)
        df_processed[f'Avg_Price_Lag_{i}'] = df_processed['Avg_Price'].shift(i)

    # Hedef Deƒüi≈üken (1 g√ºn sonraki kapanƒ±≈ü fiyatƒ±)
    df_processed['Target_Close'] = df_processed['Close'].shift(-TARGET_LAG)

    # NaN deƒüerleri temizle (√∂zellikle ilk FEATURE_LAG g√ºnleri ve g√∂stergelerin ba≈ülangƒ±cƒ±)
    # Gerekli minimum veri miktarƒ±nƒ± hesapla
    min_rows_needed = max(FEATURE_LAG, 26) + 1 # En b√ºy√ºk lag (26 for MACD) + 1 (target)
    if len(df_processed) < min_rows_needed:
        logger.warning(f"Preprocess for {asset_symbol}: NaN temizliƒüi sonrasƒ± veri bo≈ü kalabilir, minimum {min_rows_needed} satƒ±r gerekli, {len(df_processed)} mevcut.")
        st.warning(f"Se√ßilen tarih aralƒ±ƒüƒ± i√ßin yeterli veri bulunamadƒ±. L√ºtfen daha uzun bir tarih aralƒ±ƒüƒ± se√ßin (en az {min_rows_needed} g√ºn).")
        return pd.DataFrame()

    df_processed.dropna(inplace=True)
    
    if df_processed.empty:
        logger.error(f"Preprocess for {asset_symbol}: ƒ∞≈üleme sonrasƒ± DataFrame bo≈ü kaldƒ±.")
        st.error("Veri i≈üleme sonrasƒ± bo≈ü kaldƒ±. L√ºtfen veri kaynaƒüƒ±nƒ± ve tarih aralƒ±ƒüƒ±nƒ± kontrol edin.")

    logger.info(f"preprocess_data tamamlandƒ± for {asset_symbol}. Son veri boyutu: {df_processed.shape}")
    logger.debug(f"preprocess_data for {asset_symbol}: ƒ∞≈ülenmi≈ü verinin ilk 5 satƒ±rƒ±:\n{df_processed.head()}")
    logger.debug(f"preprocess_data for {asset_symbol}: ƒ∞≈ülenmi≈ü verinin son 5 satƒ±rƒ±:\n{df_processed.tail()}")
    
    return df_processed

# Model Eƒüitimi ve Tahmin
def model_training_and_prediction(df_processed: pd.DataFrame, asset_name: str, num_future_days_to_predict: int):
    """Modeli eƒüitir ve tahminler yapar."""
    logger.info(f"model_training_and_prediction √ßaƒürƒ±ldƒ± for {asset_name}. Gelen veri boyutu: {df_processed.shape}, Tahmin g√ºn sayƒ±sƒ±: {num_future_days_to_predict}")
    
    if df_processed.empty:
        st.warning(f"ƒ∞≈ülenmi≈ü veri bo≈ü, model {asset_name} i√ßin eƒüitilemiyor veya tahmin yapƒ±lamƒ±yor.")
        logger.warning(f"Model eƒüitimi/tahmini: {asset_name} i√ßin i≈ülenmi≈ü veri bo≈ü.")
        return None, None, None, None, None

    # √ñzellikler listesini tanƒ±mla
    base_features = ['Open', 'High', 'Low', 'Close', 'Volume', 'Avg_Price', 'MA7', 'MA21', 'RSI', 'MACD', 'Signal_Line', 'Daily_Return']
    lag_features = [f'{col}_Lag_{i}' for i in range(1, FEATURE_LAG + 1) for col in ['Close', 'Open', 'High', 'Low', 'Volume', 'Avg_Price']]
    features = [f for f in base_features + lag_features if f in df_processed.columns] # Sadece DataFrame'de olanlarƒ± al

    target = 'Target_Close'

    # Hedef deƒüi≈ükenin √∂zellik listesinde olmadƒ±ƒüƒ±ndan emin ol
    if target in features:
        features.remove(target)
    
    # T√ºm gerekli s√ºtunlarƒ±n DataFrame'de olduƒüundan emin ol
    missing_features = [f for f in features if f not in df_processed.columns]
    if missing_features:
        logger.error(f"Model eƒüitimi: {asset_name} i√ßin eksik √∂zellik s√ºtunlarƒ±: {missing_features}")
        st.error(f"Model eƒüitimi i√ßin gerekli bazƒ± veri s√ºtunlarƒ± eksik: {', '.join(missing_features)}. L√ºtfen veri i≈üleme adƒ±mƒ±nƒ± kontrol edin.")
        return None, None, None, None, None

    # NaN i√ßeren feature'larƒ± kontrol et
    nan_in_features = df_processed[features].isnull().sum().sum()
    if nan_in_features > 0:
        logger.error(f"Model eƒüitimi: {asset_name} i√ßin √∂zelliklerde NaN deƒüerler var. Eƒüitim durduruldu. Toplam NaN: {nan_in_features}. Detay: {df_processed[features].isnull().sum()[df_processed[features].isnull().sum() > 0].index.tolist()}")
        st.error(f"Model eƒüitimi i√ßin gerekli verilerde eksiklikler var. L√ºtfen verilerinizi kontrol edin. Eksik √ñzellikler: {df_processed[features].isnull().sum()[df_processed[features].isnull().sum() > 0].index.tolist()}")
        return None, None, None, None, None

    X = df_processed[features]
    y = df_processed[target]

    # Veriyi eƒüitim ve test setlerine ayƒ±r
    train_size = len(X) - FEATURE_LAG
    if train_size <= 0:
        st.warning(f"Eƒüitim i√ßin yeterli veri yok. En az {FEATURE_LAG + 1} g√ºn gereklidir.")
        logger.warning(f"Model eƒüitimi: {asset_name} i√ßin yeterli veri yok. {len(X)} satƒ±r var, {FEATURE_LAG} gerekli.")
        return None, None, None, None, None

    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    logger.info(f"Model Eƒüitimi ƒ∞√ßin {asset_name}: X_train boyutu: {X_train.shape}, y_train boyutu: {y_train.shape}")
    logger.info(f"Model Tahmini ƒ∞√ßin {asset_name}: X_test boyutu: {X_test.shape}, y_test boyutu: {y_test.shape}")


    # √ñl√ßekleyiciyi kaydetmek i√ßin dosya yolu
    scaler_path = "scaler.joblib"
    features_path = "features.joblib"

    try:
        # Veriyi √∂l√ßekle
        scaler = MinMaxScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        
        # √ñl√ßekleyiciyi ve √∂zellik listesini kaydet
        joblib.dump(scaler, scaler_path)
        joblib.dump(features, features_path)
        logger.info(f"√ñl√ßekleyici ve √∂zellik listesi kaydedildi: {scaler_path}, {features_path}")

        # XGBoost modelini eƒüit
        model = XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42)
        model.fit(X_train_scaled, y_train)

        # Modeli kaydet
        model_path = "xgboost_model.joblib"
        joblib.dump(model, model_path)
        logger.info(f"Model kaydedildi: {model_path}")

        # Test seti √ºzerinde tahmin yap
        y_pred_test = model.predict(X_test_scaled)

        # RMSE hesapla
        rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))
        logger.info(f"Model eƒüitimi ve tahmini tamamlandƒ± for {asset_name}. RMSE: {rmse}")

        # Gelecek Tahmini i√ßin son g√ºn verisini al
        last_data_point = df_processed[features].iloc[[-1]].copy() # Son satƒ±rƒ± DataFrame olarak al
        
        if last_data_point.empty:
            logger.error(f"Gelecek tahmin i√ßin son veri noktasƒ± {asset_name} bulunamadƒ±.")
            return model, scaler, features, None, rmse

        future_predictions = []
        current_features_df = last_data_point # ƒ∞lk tahmin i√ßin son ger√ßek veri

        for i in range(num_future_days_to_predict): # Kullanƒ±cƒ±nƒ±n belirlediƒüi g√ºn sayƒ±sƒ± kadar tahmin yap
            # √ñzellikleri √∂l√ßekle
            current_features_scaled = scaler.transform(current_features_df)
            
            # Tahmin yap
            next_day_prediction = model.predict(current_features_scaled)[0]
            future_predictions.append(next_day_prediction)
            
            # Bir sonraki tahmin i√ßin 'current_features_df'yi g√ºncelle
            new_row_dict = {}
            
            # Mevcut kapanƒ±≈ü deƒüerini al (ilk iterasyon i√ßin son ger√ßek kapanƒ±≈ü, sonrasƒ± i√ßin √∂nceki tahmin)
            current_actual_close_for_lag = current_features_df['Close'].iloc[0] 
            
            # G√ºncel Kapanƒ±≈ü, A√ßƒ±lƒ±≈ü, Y√ºksek, D√º≈ü√ºk, Hacim, Ortalama Fiyat (tahmin edilen deƒüerler)
            new_row_dict['Close'] = next_day_prediction
            new_row_dict['Open'] = next_day_prediction * (1 + np.random.uniform(-0.005, 0.005)) 
            new_row_dict['High'] = next_day_prediction * (1 + np.random.uniform(0.001, 0.01))
            new_row_dict['Low'] = next_day_prediction * (1 - np.random.uniform(0.001, 0.01))
            new_row_dict['Volume'] = current_features_df['Volume'].iloc[0] # Hacmi sabit tut (basitlik i√ßin)

            new_row_dict['Avg_Price'] = (new_row_dict['Open'] + new_row_dict['Close']) / 2
            
            # Lag √∂zelliklerini g√ºncelle (t-1'den t-FEATURE_LAG'e kadar kaydƒ±r)
            for lag in range(1, FEATURE_LAG):
                for col_name in ['Close', 'Open', 'High', 'Low', 'Volume', 'Avg_Price']:
                    new_row_dict[f'{col_name}_Lag_{lag+1}'] = current_features_df[f'{col_name}_Lag_{lag}'].iloc[0]
            
            # Lag_1 (yani bir √∂nceki g√ºn√ºn deƒüeri) ≈üimdiki unlagged deƒüerler olur
            new_row_dict['Close_Lag_1'] = current_actual_close_for_lag
            new_row_dict['Open_Lag_1'] = current_features_df['Open'].iloc[0]
            new_row_dict['High_Lag_1'] = current_features_df['High'].iloc[0]
            new_row_dict['Low_Lag_1'] = current_features_df['Low'].iloc[0]
            new_row_dict['Volume_Lag_1'] = current_features_df['Volume'].iloc[0]
            new_row_dict['Avg_Price_Lag_1'] = current_features_df['Avg_Price'].iloc[0]

            # G√∂stergeleri g√ºncelle (basit yakla≈üƒ±mlar)
            # MA'lar: Tahmin edilen kapanƒ±≈üƒ± kullanarak basit bir ≈üekilde g√ºncelle
            new_row_dict['MA7'] = (current_features_df['MA7'].iloc[0] * 6 + next_day_prediction) / 7
            new_row_dict['MA21'] = (current_features_df['MA21'].iloc[0] * 20 + next_day_prediction) / 21
            
            # RSI, MACD, Signal_Line: Bunlar i√ßin karma≈üƒ±k ge√ßmi≈ü veri serisi gerektiƒüinden, 
            # ≈üimdilik son bilinen deƒüerleri kullanacaƒüƒ±z veya basit bir eƒüilim varsayacaƒüƒ±z.
            new_row_dict['RSI'] = current_features_df['RSI'].iloc[0] if 'RSI' in current_features_df.columns and not pd.isna(current_features_df['RSI'].iloc[0]) else np.nan
            new_row_dict['MACD'] = current_features_df['MACD'].iloc[0] if 'MACD' in current_features_df.columns and not pd.isna(current_features_df['MACD'].iloc[0]) else np.nan
            new_row_dict['Signal_Line'] = current_features_df['Signal_Line'].iloc[0] if 'Signal_Line' in current_features_df.columns and not pd.isna(current_features_df['Signal_Line'].iloc[0]) else np.nan
            
            # G√ºnl√ºk Getiri
            new_row_dict['Daily_Return'] = (next_day_prediction - current_actual_close_for_lag) / current_actual_close_for_lag if current_actual_close_for_lag != 0 else 0

            # Yeni DataFrame'i olu≈ütururken indeks ve s√ºtun sƒ±rasƒ±nƒ± koru
            current_features_df = pd.DataFrame([new_row_dict], columns=features)
            
        return model, scaler, features, future_predictions, rmse

    except Exception as e:
        logger.error(f"Model eƒüitimi veya tahmini sƒ±rasƒ±nda hata olu≈ütu ({asset_name}): {e}")
        st.error(f"Model eƒüitimi veya tahmini sƒ±rasƒ±nda hata olu≈ütu: {e}. L√ºtfen log dosyasƒ±na bakƒ±n.")
        return None, None, None, None, None

# --- Streamlit Aray√ºz√º ---

# Sol s√ºtun: Varlƒ±k Se√ßimi ve Tarih Aralƒ±ƒüƒ±
with st.sidebar:
    st.header("Varlƒ±k Se√ßimi ve Ayarlar")

    # Varlƒ±k Se√ßimi
    asset_options = list(df.VARLIK_BILGILERI.keys())
    selected_asset = st.selectbox("Analiz edilecek varlƒ±ƒüƒ± se√ßin:", asset_options, key="asset_select")

    # Tarih Aralƒ±ƒüƒ±
    end_date = datetime.now()
    min_days_for_training = max(FEATURE_LAG, 26) + TARGET_LAG + 5 
    start_date_default = end_date - timedelta(days=365*5) 

    start_date_input = st.date_input("Ba≈ülangƒ±√ß Tarihi:", value=start_date_default, max_value=end_date - timedelta(days=min_days_for_training), key="start_date_input") 
    end_date_input = st.date_input("Biti≈ü Tarihi:", value=end_date, max_value=end_date, min_value=start_date_input + timedelta(days=min_days_for_training), key="end_date_input")

    if start_date_input >= end_date_input:
        st.error("Biti≈ü tarihi ba≈ülangƒ±√ß tarihinden sonra olmalƒ±dƒ±r.")
    
    if (end_date_input - start_date_input).days < min_days_for_training:
        st.warning(f"Model eƒüitimi i√ßin en az {min_days_for_training} g√ºnl√ºk veri gereklidir. L√ºtfen tarih aralƒ±ƒüƒ±nƒ± geni≈ületin.")

    # Gelecek Tahmini ƒ∞√ßin G√ºn Sayƒ±sƒ±
    prediction_days = st.number_input(
        "Gelecek ka√ß g√ºn i√ßin tahmin yapƒ±lsƒ±n?",
        min_value=1,
        max_value=30, 
        value=7,
        step=1,
        help="Modelin ka√ß g√ºn sonrasƒ±nƒ± tahmin etmesini istediƒüinizi belirtin. (1-30 g√ºn arasƒ±)"
    )

    st.markdown("---")
    st.write("Verileri √áek, Modeli Eƒüit ve Tahmin Yap")
    
    # Butona basƒ±ldƒ±ƒüƒ±nda session_state'i g√ºncelle
    if st.button("Uygulamayƒ± √áalƒ±≈ütƒ±r", use_container_width=True, key="run_button"):
        st.session_state['run_analysis'] = True
        st.session_state['selected_asset_for_run'] = selected_asset
        st.session_state['start_date_for_run'] = start_date_input
        st.session_state['end_date_for_run'] = end_date_input
        st.session_state['prediction_days_for_run'] = prediction_days
    
    # Eƒüer ilk kez a√ßƒ±lƒ±yorsa veya run_analysis False ise, hi√ßbir ≈üey yapma
    if 'run_analysis' not in st.session_state:
        st.session_state['run_analysis'] = False

# Ana B√∂l√ºm
if st.session_state.get('run_analysis', False):
    current_selected_asset = st.session_state['selected_asset_for_run']
    current_start_date = st.session_state['start_date_for_run']
    current_end_date = st.session_state['end_date_for_run']
    current_prediction_days = st.session_state['prediction_days_for_run']

    st.subheader(f"Se√ßilen Varlƒ±k: {current_selected_asset}")
    
    asset_info = df.VARLIK_BILGILERI[current_selected_asset]
    asset_symbol = asset_info["sembol"]
    asset_source = asset_info["kaynak"]

    st.info(f"'{current_selected_asset}' ({asset_symbol}) i√ßin veriler √ßekiliyor ve analiz ediliyor...")

    # datetime.combine i√ßin datetime.time.max kullanƒ±ldƒ±
    historical_data = get_historical_data(asset_symbol, asset_source, 
                                          datetime.combine(current_start_date, time.min), # time.min kullanƒ±ldƒ±
                                          datetime.combine(current_end_date, time.max)) # time.max kullanƒ±ldƒ±

    if not historical_data.empty:
        st.success(f"'{current_selected_asset}' i√ßin {len(historical_data)} g√ºnl√ºk veri ba≈üarƒ±yla √ßekildi.")
        
        # G√ºncel Fiyat Deƒüi≈üimini G√∂ster
        if len(historical_data) >= 2:
            latest_close = historical_data['Close'].iloc[-1]
            previous_close = historical_data['Close'].iloc[-2]
            price_change = latest_close - previous_close
            price_change_percent = (price_change / previous_close) * 100 if previous_close != 0 else 0

            delta_color = "inverse" if price_change < 0 else "normal"
            st.metric(
                label=f"Son Kapanƒ±≈ü Fiyatƒ± ({historical_data.index.max().strftime('%Y-%m-%d')})",
                value=f"{latest_close:.2f}",
                delta=f"{price_change:.2f} ({price_change_percent:.2f}%)",
                delta_color=delta_color
            )
        else:
            st.info("Fiyat deƒüi≈üimini g√∂stermek i√ßin yeterli ge√ßmi≈ü veri yok (en az 2 g√ºn).")

        st.write("ƒ∞lk 5 veri satƒ±rƒ±:")
        st.dataframe(historical_data.head())
        
        processed_data = preprocess_data(historical_data, asset_symbol)

        if not processed_data.empty:
            st.success("Veriler ba≈üarƒ±yla i≈ülendi ve √∂zellikler olu≈üturuldu.")
            st.write("ƒ∞≈ülenmi≈ü verinin son 5 satƒ±rƒ± (√∂zelliklerle birlikte):")
            st.dataframe(processed_data.tail())

            st.info(f"'{current_selected_asset}' i√ßin model eƒüitiliyor ve tahminler yapƒ±lƒ±yor...")
            
            model, scaler, features_list, future_predictions, rmse = model_training_and_prediction(processed_data, current_selected_asset, current_prediction_days)

            if model and future_predictions is not None:
                st.success("Model ba≈üarƒ±yla eƒüitildi ve gelecek tahminler yapƒ±ldƒ±.")
                st.metric("Model RMSE (Ortalama Kare Hata):", f"{rmse:.4f}")

                # Tahmin tarihlerini olu≈ütur
                prediction_dates = pd.date_range(start=historical_data.index.max() + timedelta(days=1), periods=current_prediction_days)
                
                # Bir sonraki g√ºn√ºn tahminini a√ßƒ±k√ßa g√∂ster
                if future_predictions:
                    next_day_date = prediction_dates[0]
                    next_day_prediction_value = future_predictions[0]
                    st.subheader(f"Yarƒ±nki Tahmini Kapanƒ±≈ü Fiyatƒ± ({next_day_date.strftime('%Y-%m-%d')}) :orange[${next_day_prediction_value:,.2f}]") # Renklendirildi
                    
                    # Y√ºzde deƒüi≈üimini hesapla ve g√∂ster
                    if not historical_data.empty and 'Close' in historical_data.columns and len(historical_data) > 0:
                        last_real_close = historical_data['Close'].iloc[-1]
                        if last_real_close != 0:
                            change_pct = ((next_day_prediction_value - last_real_close) / last_real_close) * 100
                            delta_color_pred = "inverse" if change_pct < 0 else "normal"
                            st.markdown(f"**Deƒüi≈üim:** :{'red' if change_pct < 0 else 'green'}["
                                        f"{change_pct:+.2f}% ({next_day_prediction_value - last_real_close:+.2f})] "
                                        f"{'‚¨áÔ∏è' if change_pct < 0 else '‚¨ÜÔ∏è'}")
                st.markdown("---")

                st.subheader(f"T√ºm {current_prediction_days} G√ºnl√ºk Gelecek Fiyat Tahminleri ({current_selected_asset})")
                predictions_df = pd.DataFrame({
                    "Tarih": prediction_dates,
                    "Tahmini Kapanƒ±≈ü Fiyatƒ±": future_predictions
                })
                predictions_df["Tahmini Kapanƒ±≈ü Fiyatƒ±"] = predictions_df["Tahmini Kapanƒ±≈ü Fiyatƒ±"].round(2)
                st.dataframe(predictions_df)

                # Tahminleri g√∂rselle≈ütirme
                fig = go.Figure()

                # Ge√ßmi≈ü kapanƒ±≈ü fiyatlarƒ±
                fig.add_trace(go.Scatter(x=historical_data.index, y=historical_data['Close'], mode='lines', name='Ge√ßmi≈ü Kapanƒ±≈ü Fiyatƒ±', line=dict(color='blue')))

                # Ger√ßek deƒüerler (test setinden) - sadece g√∂rselle≈ütirme i√ßin, modelin kullandƒ±ƒüƒ± ger√ßek test setidir.
                if len(processed_data) > FEATURE_LAG:
                    test_real_dates = processed_data.index[-FEATURE_LAG:]
                    test_real_values = processed_data['Close'].iloc[-FEATURE_LAG:]
                    fig.add_trace(go.Scatter(x=test_real_dates, y=test_real_values, mode='lines', name='Ger√ßek Deƒüerler (Test)', line=dict(color='green', dash='dot')))


                # Gelecek Tahminleri
                fig.add_trace(go.Scatter(x=prediction_dates, y=future_predictions, mode='lines+markers', name=f'Tahmini Fiyat ({current_prediction_days} G√ºn)', line=dict(color='red', dash='dash')))

                fig.update_layout(
                    title=f'{current_selected_asset} Fiyat Tahmini',
                    xaxis_title='Tarih',
                    yaxis_title='Fiyat',
                    hovermode='x unified',
                    legend_title="Veri Tipi",
                    height=600
                )
                st.plotly_chart(fig, use_container_width=True)

                st.subheader("Model ve √ñl√ßekleyici Yolu")
                st.write(f"- Model yolu: `xgboost_model.joblib`")
                st.write(f"- √ñl√ßekleyici yolu: `scaler.joblib`")
                st.write(f"- √ñzellikler listesi yolu: `features.joblib`")
                st.warning("Not: Bu dosyalar uygulamanƒ±n √ßalƒ±≈ütƒ±ƒüƒ± dizinde olu≈üturulur.")

            else:
                st.error("Model eƒüitimi veya tahmini sƒ±rasƒ±nda bir sorun olu≈ütu. L√ºtfen log dosyasƒ±na bakƒ±n.")
        else:
            st.error("Veri i≈üleme sonrasƒ± bo≈ü kaldƒ±. L√ºtfen veri kaynaƒüƒ±nƒ± ve tarih aralƒ±ƒüƒ±nƒ± kontrol edin.")
    else:
        st.error(f"'{current_selected_asset}' i√ßin ge√ßmi≈ü veri √ßekilemedi. L√ºtfen internet baƒülantƒ±nƒ±zƒ± veya se√ßilen varlƒ±ƒüƒ±n sembol√ºn√º/kaynaƒüƒ±nƒ± kontrol edin. Log dosyasƒ±nda daha fazla detay bulabilirsiniz.")
